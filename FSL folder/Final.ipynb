{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed684f96-6ddc-4e27-ab38-dfde3f4ec517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3322462b-9961-48f2-b3f5-4937278e2547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load of all the .csv files containing extracted image features\n",
    "data_USG = pd.read_csv(\"Features/Ultrasound features.csv\")\n",
    "data_MMG = pd.read_csv(\"Features/Mammogram features.csv\")\n",
    "data_multimodal = pd.read_csv(\"Features/multimodal features.csv\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8098d21d-13ab-42aa-bea7-b9c5438030aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For one hot encoding the labels this tab is required\n",
    "class_mapping = {\n",
    "    'B': 0,\n",
    "    'M': 1,\n",
    "}\n",
    "num_classes = 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_USG['Class'] = label_encoder.fit_transform(data_USG['Class'])\n",
    "data_MMG['Class'] = label_encoder.fit_transform(data_MMG['Class'])\n",
    "data_multimodal['Class'] = label_encoder.fit_transform(data_multimodal['Class'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "179ccd87-23dd-475d-9e90-c1df1c201c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=2):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.multihead_attention = nn.MultiheadAttention(input_dim, num_heads)\n",
    "        \n",
    "        # Feedforward neural network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization\n",
    "        x_norm = self.layer_norm1(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention_output, _ = self.multihead_attention(x_norm, x_norm,x_norm)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        x_residual = x + attention_output\n",
    "        x_norm2 = self.layer_norm2(x_residual)\n",
    "        \n",
    "        # Feedforward network\n",
    "        output = self.feedforward(x_norm2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16e5115f-87c3-4d50-824e-02340bb54e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SNAIL(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_blocks, num_heads=2):\n",
    "        super(SNAIL, self).__init__()\n",
    "        \n",
    "        # Initialize AttentionBlocks\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            AttentionBlock(input_dim, output_dim, num_heads)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(input_dim, 1)  # Binary classification, so output_dim is 1\n",
    "        \n",
    "        # Sigmoid activation for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        self.loss_function = nn.BCELoss()\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        for i, attention_block in enumerate(self.attention_blocks):\n",
    "            # Forward pass through the AttentionBlock\n",
    "            x = attention_block(x)\n",
    "            \n",
    "            # Calculate logits\n",
    "            logits = self.classification_layer(x)\n",
    "            \n",
    "            # Apply sigmoid activation\n",
    "            predictions = self.sigmoid(logits)\n",
    "            \n",
    "            # Calculate binary cross-entropy loss\n",
    "            loss = self.loss_function(predictions, target)\n",
    "            \n",
    "            # Backpropagation and parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the learning rate (optional)\n",
    "            self.learning_rate = calculate_new_learning_rate(self.learning_rate, loss)\n",
    "            \n",
    "            print(f\"Iteration {i + 1}: Loss = {loss.item()}\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f265612-e2bd-4044-ab02-77f63315a849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_new_learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43msnail_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Calculate loss and perform backpropagation\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         loss \u001b[38;5;241m=\u001b[39m snail_model\u001b[38;5;241m.\u001b[39mloss_function(predictions, batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 46\u001b[0m, in \u001b[0;36mSNAIL.forward\u001b[1;34m(self, x, target)\u001b[0m\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Update the learning rate (optional)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_new_learning_rate\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, loss)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calculate_new_learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract features and labels\n",
    "X = data_USG.drop(columns=['Class']).values\n",
    "y = data_USG['Class'].values\n",
    "\n",
    "# Split your data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Assuming 'y_train_tensor' and 'y_val_tensor' have shapes [batch_size]\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_val_tensor = y_val_tensor.view(-1, 1)\n",
    "\n",
    "# Initialize your SNAIL model\n",
    "input_dim = 512  # Adjust to your input dimension\n",
    "output_dim = 512  # Output dimension of AttentionBlock\n",
    "num_blocks = 3  # Number of AttentionBlocks\n",
    "snail_model = SNAIL(input_dim, output_dim, num_blocks)\n",
    "\n",
    "# Define a data loader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        predictions = snail_model(batch_X, batch_y)\n",
    "\n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = snail_model.loss_function(predictions, batch_y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model after each epoch (optional)\n",
    "    with torch.no_grad():\n",
    "        val_predictions = snail_model(X_val_tensor, y_val_tensor)\n",
    "        val_loss = snail_model.loss_function(val_predictions, y_val_tensor)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss.item()}\")\n",
    "    \n",
    "# Optionally, save the trained model\n",
    "torch.save(snail_model.state_dict(), 'snail_model.pth')\n",
    "\n",
    "# Load the trained model\n",
    "snail_model.load_state_dict(torch.load('snail_model.pth'))\n",
    "\n",
    "# Convert your test data (if available) to PyTorch tensors\n",
    "X_test_tensor = torch.FloatTensor(X_test)  # Replace 'X_test' with your test data\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    test_predictions = snail_model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eccb792-30a5-4fff-8a48-c75b70b683d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape - X: torch.Size([32, 512]), y: torch.Size([32, 1])\n",
      "Total number of batches: 13\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the DataLoader to inspect batch shapes\n",
    "for batch_X, batch_y in train_loader:\n",
    "    batch_size, input_dim = batch_X.shape\n",
    "    print(f\"Batch Shape - X: {batch_X.shape}, y: {batch_y.shape}\")\n",
    "    break  # Break after inspecting the first batch\n",
    "\n",
    "# Optionally, you can also print the total number of batches\n",
    "print(f\"Total number of batches: {len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
