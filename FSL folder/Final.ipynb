{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed684f96-6ddc-4e27-ab38-dfde3f4ec517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179ccd87-23dd-475d-9e90-c1df1c201c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x15e1db68940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=1):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.multihead_attention = nn.MultiheadAttention(input_dim, num_heads)\n",
    "        \n",
    "        # Feedforward neural network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization\n",
    "        x_norm = self.layer_norm1(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        attention_output, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        x_residual = x + attention_output\n",
    "        x_norm2 = self.layer_norm2(x_residual)\n",
    "        \n",
    "        # Feedforward network\n",
    "        output = self.feedforward(x_norm2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Disable anomaly detection when done (optional)\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e5115f-87c3-4d50-824e-02340bb54e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x15e201638b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class SNAIL(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_blocks, num_heads=1):\n",
    "        super(SNAIL, self).__init__()\n",
    "        \n",
    "        # Initialize AttentionBlocks\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            AttentionBlock(input_dim, output_dim, num_heads)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(input_dim, 1)  # Binary classification, so output_dim is 1\n",
    "        \n",
    "        # Sigmoid activation for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        self.loss_function = nn.BCELoss()\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        for i, attention_block in enumerate(self.attention_blocks):\n",
    "            # Forward pass through the AttentionBlock\n",
    "            x = attention_block(x)\n",
    "            \n",
    "            # Calculate logits\n",
    "            logits = self.classification_layer(x)\n",
    "            \n",
    "            # Apply sigmoid activation\n",
    "            predictions = self.sigmoid(logits)\n",
    "            \n",
    "            # Calculate binary cross-entropy loss\n",
    "            loss = self.loss_function(predictions, target)\n",
    "            \n",
    "            # Backpropagation and parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the learning rate (optional)\n",
    "            # self.learning_rate = calculate_new_learning_rate(self.learning_rate, loss)\n",
    "            \n",
    "            print(f\"Iteration {i + 1}: Loss = {loss.item()}\")\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "# Disable anomaly detection when done (optional)\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3322462b-9961-48f2-b3f5-4937278e2547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load of all the .csv files containing extracted image features\n",
    "data_USG = pd.read_csv(\"Features/Ultrasound features.csv\")\n",
    "data_MMG = pd.read_csv(\"Features/Mammogram features.csv\")\n",
    "data_multimodal = pd.read_csv(\"Features/multimodal features.csv\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8098d21d-13ab-42aa-bea7-b9c5438030aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For one hot encoding the labels this tab is required\n",
    "class_mapping = {\n",
    "    'B': 0,\n",
    "    'M': 1,\n",
    "}\n",
    "num_classes = 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_USG['Class'] = label_encoder.fit_transform(data_USG['Class'])\n",
    "data_MMG['Class'] = label_encoder.fit_transform(data_MMG['Class'])\n",
    "data_multimodal['Class'] = label_encoder.fit_transform(data_multimodal['Class'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f265612-e2bd-4044-ab02-77f63315a849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 0.6896322965621948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\vishu\\AppData\\Local\\Temp\\ipykernel_12916\\2675193636.py\", line 37, in <module>\n",
      "    predictions = snail_model(batch_X, batch_y)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\vishu\\AppData\\Local\\Temp\\ipykernel_12916\\23024762.py\", line 32, in forward\n",
      "    x = attention_block(x)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\vishu\\AppData\\Local\\Temp\\ipykernel_12916\\3733391300.py\", line 34, in forward\n",
      "    output = self.feedforward(x_norm2)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\vishu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [512, 512]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43msnail_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# Calculate loss and perform backpropagation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         loss \u001b[38;5;241m=\u001b[39m snail_model\u001b[38;5;241m.\u001b[39mloss_function(predictions, batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mSNAIL.forward\u001b[1;34m(self, x, target)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Backpropagation and parameter update\u001b[39;00m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Update the learning rate (optional)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# self.learning_rate = calculate_new_learning_rate(self.learning_rate, loss)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [512, 512]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data_USG.drop(columns=['Class']).values\n",
    "y = data_USG['Class'].values\n",
    "\n",
    "# Split your data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "# Assuming 'y_train_tensor' and 'y_val_tensor' have shapes [batch_size]\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_val_tensor = y_val_tensor.view(-1, 1)\n",
    "\n",
    "# Initialize your SNAIL model\n",
    "input_dim = 512  # Adjust to your input dimension\n",
    "output_dim = 512  # Output dimension of AttentionBlock\n",
    "num_blocks = 3  # Number of AttentionBlocks\n",
    "snail_model = SNAIL(input_dim, output_dim, num_blocks)\n",
    "\n",
    "# Define a data loader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        predictions = snail_model(batch_X, batch_y)\n",
    "\n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = snail_model.loss_function(predictions, batch_y.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model after each epoch (optional)\n",
    "    with torch.no_grad():\n",
    "        val_predictions = snail_model(X_val_tensor, y_val_tensor)\n",
    "        val_loss = snail_model.loss_function(val_predictions, y_val_tensor)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss.item()}\")\n",
    "    \n",
    "# Optionally, save the trained model\n",
    "torch.save(snail_model.state_dict(), 'snail_model.pth')\n",
    "\n",
    "# Load the trained model\n",
    "snail_model.load_state_dict(torch.load('snail_model.pth'))\n",
    "\n",
    "# Convert your test data (if available) to PyTorch tensors\n",
    "X_test_tensor = torch.FloatTensor(X_test)  # Replace 'X_test' with your test data\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    test_predictions = snail_model(X_test_tensor)\n",
    "    \n",
    "    \n",
    "# Disable anomaly detection when done (optional)\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccb792-30a5-4fff-8a48-c75b70b683d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
