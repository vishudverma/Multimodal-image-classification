{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b417b7-a369-4abf-b24b-a0881e5006be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c979f875-f236-41e8-a09f-0210ec0f2851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_USG = pd.read_csv(\"Features/Ultrasound features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ebd0634-9a71-4b58-a19d-e33bb37c614e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "  'B': 0,\n",
    "  'M': 1,\n",
    "}\n",
    "num_classes = 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_USG['Class'] = label_encoder.fit_transform(data_USG['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc27e2ca-99c6-4aeb-a95e-e8e6f01e45b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_usg_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_usg_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, query_usg_features[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m, in \u001b[0;36mSnailFewShotModel.forward\u001b[1;34m(self, support_features, query_features)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, support_features, query_features):\n\u001b[1;32m---> 26\u001b[0m     weighted_support_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(weighted_support_features)\n\u001b[0;32m     28\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[1;34m(self, support_features, query_features)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, support_features, query_features):\n\u001b[1;32m---> 12\u001b[0m     concatenated_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfnn(concatenated_features), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     weighted_support_features \u001b[38;5;241m=\u001b[39m attention_weights \u001b[38;5;241m*\u001b[39m support_features\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, support_features_dim, query_features_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(support_features_dim + query_features_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, support_features, query_features):\n",
    "        concatenated_features = torch.cat((support_features, query_features), dim=1)\n",
    "        attention_weights = torch.softmax(self.fnn(concatenated_features), dim=1)\n",
    "        weighted_support_features = attention_weights * support_features\n",
    "\n",
    "        return weighted_support_features\n",
    "\n",
    "class SnailFewShotModel(nn.Module):\n",
    "    def __init__(self, support_features_dim, query_features_dim, num_classes):\n",
    "        super(SnailFewShotModel, self).__init__()\n",
    "\n",
    "        self.attention_block = AttentionBlock(support_features_dim, query_features_dim)\n",
    "        self.classifier = nn.Linear(support_features_dim, num_classes)\n",
    "\n",
    "    def forward(self, support_features, query_features):\n",
    "        weighted_support_features = self.attention_block(support_features, query_features)\n",
    "        logits = self.classifier(weighted_support_features)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        return probs\n",
    "\n",
    "# Convert the ultrasound features to float32\n",
    "usg_features = np.array(data_USG.values, dtype=np.float32)\n",
    "\n",
    "# Split the ultrasound features into support set, query set, and evaluation set\n",
    "support_usg_features = usg_features[:32]\n",
    "query_usg_features = usg_features[32:96]\n",
    "evaluation_usg_features = usg_features[96:]\n",
    "\n",
    "# Create the model\n",
    "model = SnailFewShotModel(support_usg_features.size, query_usg_features.size, 2)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate the loss\n",
    "    logits = model(support_usg_features, query_usg_features)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, query_usg_features[:, -1])\n",
    "\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_logits = model(support_usg_features, evaluation_usg_features)\n",
    "evaluation_preds = torch.argmax(evaluation_logits, dim=1)\n",
    "evaluation_accuracy = (evaluation_preds == evaluation_usg_features[:, -1]).float().mean()\n",
    "\n",
    "print('Evaluation accuracy:', evaluation_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3fed1-77fc-486d-9f09-d31949c350ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
