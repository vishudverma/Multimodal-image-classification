{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8934207-779c-456a-8999-a19f1e7802ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import math\n",
    "import torch\n",
    "import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d7613c-8e64-4dbb-9da9-cc77f7e297f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load of all the .csv files containing extracted image features\n",
    "data_USG = pd.read_csv(\"Features/Ultrasound features.csv\")\n",
    "data_MMG = pd.read_csv(\"Features/Mammogram features.csv\")\n",
    "data_multimodal = pd.read_csv(\"Features/multimodal features.csv\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dd55e6-9cca-4025-986f-54f985a96f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#For one hot encoding the labels this tab is required\n",
    "class_mapping = {\n",
    "    'B': 0,\n",
    "    'M': 1,\n",
    "}\n",
    "num_classes = 2\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_USG['Class'] = label_encoder.fit_transform(data_USG['Class'])\n",
    "data_MMG['Class'] = label_encoder.fit_transform(data_MMG['Class'])\n",
    "data_multimodal['Class'] = label_encoder.fit_transform(data_multimodal['Class'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea0cff-a0c5-45ad-8bd9-987fa851424f",
   "metadata": {},
   "source": [
    "TCBlock retains information and details(weights) from the bool of information(Temporal Convolutions) it is not being used as there is no sequential data in our files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af170c2a-00b0-474d-82c6-30e4fd4e7a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Define TCBlock \n",
    "# class TCBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, seq_length, filters):\n",
    "#         super(TCBlock, self).__init__()\n",
    "#         self.dense_blocks = nn.ModuleList([DenseBlock(in_channels + i * filters, 2 ** (i+1), filters)\n",
    "#                                            for i in range(int(math.ceil(math.log(seq_length, 2))))])\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # input is dimensions (N, T, in_channels)\n",
    "#         input = torch.transpose(input, 1, 2)\n",
    "#         for block in self.dense_blocks:\n",
    "#             input = block(input)\n",
    "#         return torch.transpose(input, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c0b92-c3ea-495c-8c74-ced9e75ae773",
   "metadata": {},
   "source": [
    "Attention Block pin points information in a seemingly large set of data and retains it to pass on to the temporal Convolution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a77576-3746-4536-8c8f-13fcc8ff11f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, key_size, value_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.linear_query = nn.Linear(in_channels, key_size)\n",
    "        self.linear_keys = nn.Linear(in_channels, key_size)\n",
    "        self.linear_values = nn.Linear(in_channels, value_size)\n",
    "        self.sqrt_key_size = math.sqrt(key_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is dim (N, T, in_channels) where N is the batch_size, and T is\n",
    "        # the sequence length\n",
    "        batch_size, seq_length, in_channels = input.size()\n",
    "        \n",
    "        # Create an upper triangular mask\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length, device=input.device), diagonal=1).byte()\n",
    "        \n",
    "        keys = self.linear_keys(input)  # shape: (N, T, key_size)\n",
    "        query = self.linear_query(input)  # shape: (N, T, key_size)\n",
    "        values = self.linear_values(input)  # shape: (N, T, value_size)\n",
    "        \n",
    "        temp = torch.bmm(query, keys.transpose(1, 2))  # shape: (N, T, T)\n",
    "        temp.data.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "        temp = F.softmax(temp / self.sqrt_key_size, dim=1)  # shape: (N, T, T)\n",
    "        temp = torch.bmm(temp, values)  # shape: (N, T, value_size)\n",
    "        \n",
    "        return torch.cat((input, temp), dim=2)  # shape: (N, T, in_channels + value_size)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff7da1-5af8-44e4-bf50-8c3c34fa4a1b",
   "metadata": {},
   "source": [
    "Define the SNAIL architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a47834e-919c-4bd1-9697-e9cddae8737a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class SnailFewShot(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SnailFewShot, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # Adjust the hidden layer size as needed\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),  # You can adjust the hidden layer size here as well\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention = AttentionBlock(32, 8, 8)  # Adjust key_size and value_size as needed\n",
    "        self.fc = nn.Linear(32 + 8, 1)  # Output a single value for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation for binary classification\n",
    "\n",
    "# Create instances of the SnailFewShot model for both input shapes\n",
    "input_dim_512 = 512  # Number of features in the (512, 0) input\n",
    "model_512 = SnailFewShot(input_dim_512)\n",
    "\n",
    "input_dim_1024 = 1024  # Number of features in the (1024, 0) input\n",
    "model_1024 = SnailFewShot(input_dim_1024)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e2a79-4cf6-4ead-8040-a85d0a8999a9",
   "metadata": {},
   "source": [
    "From this point on the data is run and the main block is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f8e2a4-302c-487b-9709-3c7aad7af7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # ... (Data Preparation)\n",
    "\n",
    "# # Initialize the model\n",
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "# model = SnailFewShot(input_dim)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10  # Adjust as needed\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         predictions = model(batch_X)\n",
    "#         loss = criterion(predictions, batch_y.view(-1, 1))  # Ensure y has shape (batch_size, 1)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# # Evaluation\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for batch_X, batch_y in test_loader:\n",
    "#         predictions = model(batch_X)\n",
    "#         predicted_labels = (predictions >= 0.5).float()\n",
    "#         total += batch_y.size(0)\n",
    "#         correct += (predicted_labels == batch_y.view(-1, 1)).sum().item()  # Ensure y has shape (batch_size, 1)\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4886d8fd-0a59-4add-af27-61bb1fb53460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 41\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Ensure y has shape (batch_size, 1)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mSnailFewShot.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# input is dim (N, T, in_channels) where N is the batch_size, and T is\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# the sequence length\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     batch_size, seq_length, in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Create an upper triangular mask\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(seq_length, seq_length, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Extract features and labels\n",
    "X_USG = data_USG.drop(columns=['Class']).values\n",
    "y_USG = data_USG['Class'].values  # Assuming labels are 0 or 1 for binary classification\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_USG, y_USG, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = SnailFewShot(input_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y.view(-1, 1))  # Ensure y has shape (batch_size, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        predicted_labels = (predictions >= 0.5).float()\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted_labels == batch_y.view(-1, 1)).sum().item()  # Ensure y has shape (batch_size, 1)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04a14b-5999-4ad7-9d5b-af57e871d20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and get predictions\n",
    "model.eval()\n",
    "model_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        model_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "predicted_labels = (np.array(model_predictions) >= 0.5).astype(int)\n",
    "\n",
    "# Convert true labels to a numpy array\n",
    "true_labels = y_test_tensor.cpu().numpy().astype(int)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d636a-e245-4b7f-9515-371700fb560b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X_MMG = data_MMG.drop(columns=['Class']).values\n",
    "y_MMG = data_MMG['Class'].values  # Assuming labels are 0 or 1 for binary classification\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_MMG, y_MMG, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = SnailFewShot(input_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        predicted_labels = (predictions >= 0.5).float()\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted_labels == batch_y).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa4965-7f6f-4391-9c8f-551be92285ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and get predictions\n",
    "model.eval()\n",
    "model_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        model_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "predicted_labels = (np.array(model_predictions) >= 0.5).astype(int)\n",
    "\n",
    "# Convert true labels to a numpy array\n",
    "true_labels = y_test_tensor.cpu().numpy().astype(int)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7a7b1-4ff0-426e-9efb-936391a50b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = data_multimodal.drop(columns=['Class']).values\n",
    "y = data_multimodal['Class'].values  # Assuming labels are 0 or 1 for binary classification\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "batch_size = 64  # Adjust as needed\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = SnailFewShot(input_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        predicted_labels = (predictions >= 0.5).float()\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted_labels == batch_y).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e699b44-f540-4996-b484-7f50954a612f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and get predictions\n",
    "model.eval()\n",
    "model_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        model_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "predicted_labels = (np.array(model_predictions) >= 0.5).astype(int)\n",
    "\n",
    "# Convert true labels to a numpy array\n",
    "true_labels = y_test_tensor.cpu().numpy().astype(int)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b01538-0031-4c29-b03f-e26569de6377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
